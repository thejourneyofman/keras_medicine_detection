{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/thejourneyofman/keras_medicine_detection/blob/master/02_fineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0XNU5qVjZ_k0",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598.0
    },
    "outputId": "5596093c-44e2-4b5f-feeb-8bdf3164309f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16720 images belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0828 12:48:34.737049 140102370645888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0828 12:48:34.776527 140102370645888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0828 12:48:34.783527 140102370645888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0828 12:48:34.826685 140102370645888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 images belonging to 10 classes.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 5s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0828 12:48:41.415245 140102370645888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0828 12:48:41.416412 140102370645888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0828 12:48:45.319582 140102370645888 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:80: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"se...)`\n",
      "W0828 12:48:45.499446 140102370645888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., steps_per_epoch=522, epochs=50, validation_steps=5000)`\n",
      "W0828 12:48:45.617300 140102370645888 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training head...\n",
      "Epoch 1/50\n",
      "170/522 [========>.....................] - ETA: 1:11:10 - loss: 2.2790 - acc: 0.1824"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "\n",
    "classes = ['medicine00', 'medicine01', 'medicine02', 'medicine03', 'medicine04',\n",
    "           'medicine05', 'medicine06', 'medicine07', 'medicine08', 'medicine09']\n",
    "nb_classes = len(classes)\n",
    "\n",
    "batch_size = 32\n",
    "img_rows, img_cols = 150, 150\n",
    "channels = 3\n",
    "\n",
    "train_data_dir = 'train'\n",
    "validation_data_dir = 'test'\n",
    "\n",
    "nb_train_samples = 16720\n",
    "nb_val_samples = 5000\n",
    "nb_epoch = 50\n",
    "\n",
    "result_dir = 'results'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.mkdir(result_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # initialize the training data augmentation object\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0 / 255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    # initialize the validation/testing data augmentation object\n",
    "    test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "    # initialize the training generator\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        color_mode='rgb',\n",
    "        classes=classes,\n",
    "        class_mode='categorical',\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    # initialize the validation generator\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        color_mode='rgb',\n",
    "        classes=classes,\n",
    "        class_mode='categorical',\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    # load the VGG16 network, ensuring the head FC layer sets are left\n",
    "    # off\n",
    "    input_tensor = Input(shape=(img_rows, img_cols, 3))\n",
    "    vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "    # vgg16.summary()\n",
    "\n",
    "    # construct the head of the model that will be placed on top of the\n",
    "    # the base model\n",
    "    head_model = Sequential()\n",
    "    head_model.add(Flatten(input_shape=vgg16.output_shape[1:]))\n",
    "    head_model.add(Dense(256, activation='relu'))\n",
    "    head_model.add(Dropout(0.5))\n",
    "    head_model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "    # Load the trained weights of FC layer\n",
    "    # top_model.load_weights(os.path.join(result_dir, 'bottleneck_fc_model.h5'))\n",
    "\n",
    "    # place the head FC model on top of the base model (this will become\n",
    "    # the actual model we will train)\n",
    "    model = Model(input=vgg16.input, output=head_model(vgg16.output))\n",
    "\n",
    "    # loop over all layers in the base model and freeze them so they will\n",
    "    # *not* be updated during the first training process\n",
    "    for layer in model.layers[:15]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # compile our model (this needs to be done after our setting our\n",
    "    # layers to being non-trainable\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    # Fine-tuning\n",
    "    print(\"[INFO] training head...\")\n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_val_samples)\n",
    "\n",
    "    # serialize and save the model to disk\n",
    "    model.save_weights(os.path.join(result_dir, 'finetuning.h5'))\n",
    "    loss = history.history['loss']\n",
    "    acc = history.history['acc']\n",
    "    val_loss = history.history['val_loss']\n",
    "    val_acc = history.history['val_acc']\n",
    "    nb_epoch = len(acc)\n",
    "\n",
    "    with open(os.path.join(result_dir, 'history_finetuning.txt'), \"w\") as fp:\n",
    "        fp.write(\"epoch\\tloss\\tacc\\tval_loss\\tval_acc\\n\")\n",
    "        for i in range(nb_epoch):\n",
    "            fp.write(\"%d\\t%f\\t%f\\t%f\\t%f\\n\" % (i, loss[i], acc[i], val_loss[i], val_acc[i]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_fineTuning.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}