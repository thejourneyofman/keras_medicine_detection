{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/thejourneyofman/keras_medicine_detection/blob/master/02_fineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0XNU5qVjZ_k0",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    },
    "outputId": "5596093c-44e2-4b5f-feeb-8bdf3164309f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16720 images belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0828 12:48:34.737049 140102370645888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0828 12:48:34.776527 140102370645888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0828 12:48:34.783527 140102370645888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0828 12:48:34.826685 140102370645888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 images belonging to 10 classes.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 5s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0828 12:48:41.415245 140102370645888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0828 12:48:41.416412 140102370645888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0828 12:48:45.319582 140102370645888 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:80: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"se...)`\n",
      "W0828 12:48:45.499446 140102370645888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., steps_per_epoch=522, epochs=50, validation_steps=5000)`\n",
      "W0828 12:48:45.617300 140102370645888 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training head...\n",
      "Epoch 1/50\n",
      "522/522 [==============================] - 9018s 17s/step - loss: 1.8719 - acc: 0.3430 - val_loss: 1.2022 - val_acc: 0.5828\n",
      "Epoch 2/50\n",
      "522/522 [==============================] - 903s 2s/step - loss: 1.0891 - acc: 0.6170 - val_loss: 0.7805 - val_acc: 0.7378\n",
      "Epoch 3/50\n",
      "522/522 [==============================] - 901s 2s/step - loss: 0.8039 - acc: 0.7153 - val_loss: 0.5910 - val_acc: 0.8111\n",
      "Epoch 4/50\n",
      "522/522 [==============================] - 901s 2s/step - loss: 0.6374 - acc: 0.7797 - val_loss: 0.4594 - val_acc: 0.8539\n",
      "Epoch 5/50\n",
      "522/522 [==============================] - 899s 2s/step - loss: 0.5179 - acc: 0.8233 - val_loss: 0.3765 - val_acc: 0.8805\n",
      "Epoch 6/50\n",
      "522/522 [==============================] - 898s 2s/step - loss: 0.4406 - acc: 0.8467 - val_loss: 0.3150 - val_acc: 0.8929\n",
      "Epoch 7/50\n",
      "522/522 [==============================] - 898s 2s/step - loss: 0.3867 - acc: 0.8684 - val_loss: 0.2778 - val_acc: 0.9162\n",
      "Epoch 8/50\n",
      "522/522 [==============================] - 901s 2s/step - loss: 0.3410 - acc: 0.8849 - val_loss: 0.2269 - val_acc: 0.9297\n",
      "Epoch 9/50\n",
      "522/522 [==============================] - 899s 2s/step - loss: 0.2952 - acc: 0.9004 - val_loss: 0.2278 - val_acc: 0.9230\n",
      "Epoch 10/50\n",
      "522/522 [==============================] - 898s 2s/step - loss: 0.2661 - acc: 0.9103 - val_loss: 0.2278 - val_acc: 0.9283\n",
      "Epoch 11/50\n",
      "522/522 [==============================] - 898s 2s/step - loss: 0.2516 - acc: 0.9142 - val_loss: 0.1685 - val_acc: 0.9481\n",
      "Epoch 12/50\n",
      "522/522 [==============================] - 900s 2s/step - loss: 0.2204 - acc: 0.9273 - val_loss: 0.1582 - val_acc: 0.9509\n",
      "Epoch 13/50\n",
      "522/522 [==============================] - 899s 2s/step - loss: 0.1972 - acc: 0.9360 - val_loss: 0.1670 - val_acc: 0.9466\n",
      "Epoch 14/50\n",
      "522/522 [==============================] - 894s 2s/step - loss: 0.1889 - acc: 0.9362 - val_loss: 0.1505 - val_acc: 0.9520\n",
      "Epoch 15/50\n",
      "522/522 [==============================] - 894s 2s/step - loss: 0.1710 - acc: 0.9428 - val_loss: 0.1219 - val_acc: 0.9615\n",
      "Epoch 16/50\n",
      "522/522 [==============================] - 896s 2s/step - loss: 0.1554 - acc: 0.9483 - val_loss: 0.1258 - val_acc: 0.9584\n",
      "Epoch 17/50\n",
      "522/522 [==============================] - 893s 2s/step - loss: 0.1516 - acc: 0.9502 - val_loss: 0.1016 - val_acc: 0.9678\n",
      "Epoch 18/50\n",
      "522/522 [==============================] - 890s 2s/step - loss: 0.1412 - acc: 0.9534 - val_loss: 0.1276 - val_acc: 0.9579\n",
      "Epoch 19/50\n",
      "522/522 [==============================] - 889s 2s/step - loss: 0.1297 - acc: 0.9576 - val_loss: 0.1003 - val_acc: 0.9666\n",
      "Epoch 20/50\n",
      "522/522 [==============================] - 888s 2s/step - loss: 0.1232 - acc: 0.9604 - val_loss: 0.0903 - val_acc: 0.9702\n",
      "Epoch 21/50\n",
      "522/522 [==============================] - 885s 2s/step - loss: 0.1110 - acc: 0.9638 - val_loss: 0.1008 - val_acc: 0.9652\n",
      "Epoch 22/50\n",
      "522/522 [==============================] - 884s 2s/step - loss: 0.1100 - acc: 0.9646 - val_loss: 0.0842 - val_acc: 0.9738\n",
      "Epoch 23/50\n",
      "522/522 [==============================] - 884s 2s/step - loss: 0.1005 - acc: 0.9655 - val_loss: 0.0822 - val_acc: 0.9724\n",
      "Epoch 24/50\n",
      "522/522 [==============================] - 882s 2s/step - loss: 0.0963 - acc: 0.9677 - val_loss: 0.0838 - val_acc: 0.9720\n",
      "Epoch 25/50\n",
      "522/522 [==============================] - 882s 2s/step - loss: 0.0880 - acc: 0.9711 - val_loss: 0.0908 - val_acc: 0.9675\n",
      "Epoch 26/50\n",
      "522/522 [==============================] - 881s 2s/step - loss: 0.0850 - acc: 0.9728 - val_loss: 0.0669 - val_acc: 0.9768\n",
      "Epoch 27/50\n",
      "522/522 [==============================] - 881s 2s/step - loss: 0.0844 - acc: 0.9725 - val_loss: 0.0867 - val_acc: 0.9688\n",
      "Epoch 28/50\n",
      "522/522 [==============================] - 881s 2s/step - loss: 0.0806 - acc: 0.9738 - val_loss: 0.0733 - val_acc: 0.9754\n",
      "Epoch 29/50\n",
      "522/522 [==============================] - 880s 2s/step - loss: 0.0781 - acc: 0.9747 - val_loss: 0.0820 - val_acc: 0.9711\n",
      "Epoch 30/50\n",
      "522/522 [==============================] - 881s 2s/step - loss: 0.0705 - acc: 0.9772 - val_loss: 0.0855 - val_acc: 0.9703\n",
      "Epoch 31/50\n",
      "522/522 [==============================] - 880s 2s/step - loss: 0.0690 - acc: 0.9778 - val_loss: 0.0679 - val_acc: 0.9776\n",
      "Epoch 32/50\n",
      "521/522 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9795"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "\n",
    "classes = ['medicine00', 'medicine01', 'medicine02', 'medicine03', 'medicine04',\n",
    "           'medicine05', 'medicine06', 'medicine07', 'medicine08', 'medicine09']\n",
    "nb_classes = len(classes)\n",
    "\n",
    "batch_size = 32\n",
    "img_rows, img_cols = 150, 150\n",
    "channels = 3\n",
    "\n",
    "train_data_dir = 'train'\n",
    "validation_data_dir = 'test'\n",
    "\n",
    "nb_train_samples = 16720\n",
    "nb_val_samples = 5000\n",
    "nb_epoch = 50\n",
    "\n",
    "result_dir = 'results'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.mkdir(result_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # initialize the training data augmentation object\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0 / 255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    # initialize the validation/testing data augmentation object\n",
    "    test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "    # initialize the training generator\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        color_mode='rgb',\n",
    "        classes=classes,\n",
    "        class_mode='categorical',\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    # initialize the validation generator\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        color_mode='rgb',\n",
    "        classes=classes,\n",
    "        class_mode='categorical',\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    # load the VGG16 network, ensuring the head FC layer sets are left\n",
    "    # off\n",
    "    input_tensor = Input(shape=(img_rows, img_cols, 3))\n",
    "    vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "    # vgg16.summary()\n",
    "\n",
    "    # construct the head of the model that will be placed on top of the\n",
    "    # the base model\n",
    "    head_model = Sequential()\n",
    "    head_model.add(Flatten(input_shape=vgg16.output_shape[1:]))\n",
    "    head_model.add(Dense(256, activation='relu'))\n",
    "    head_model.add(Dropout(0.5))\n",
    "    head_model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "    # Load the trained weights of FC layer\n",
    "    # top_model.load_weights(os.path.join(result_dir, 'bottleneck_fc_model.h5'))\n",
    "\n",
    "    # place the head FC model on top of the base model (this will become\n",
    "    # the actual model we will train)\n",
    "    model = Model(input=vgg16.input, output=head_model(vgg16.output))\n",
    "\n",
    "    # loop over all layers in the base model and freeze them so they will\n",
    "    # *not* be updated during the first training process\n",
    "    for layer in model.layers[:15]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # compile our model (this needs to be done after our setting our\n",
    "    # layers to being non-trainable\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    # Fine-tuning\n",
    "    print(\"[INFO] training head...\")\n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_val_samples)\n",
    "\n",
    "    # serialize and save the model to disk\n",
    "    model.save_weights(os.path.join(result_dir, 'finetuning.h5'))\n",
    "    loss = history.history['loss']\n",
    "    acc = history.history['acc']\n",
    "    val_loss = history.history['val_loss']\n",
    "    val_acc = history.history['val_acc']\n",
    "    nb_epoch = len(acc)\n",
    "\n",
    "    with open(os.path.join(result_dir, 'history_finetuning.txt'), \"w\") as fp:\n",
    "        fp.write(\"epoch\\tloss\\tacc\\tval_loss\\tval_acc\\n\")\n",
    "        for i in range(nb_epoch):\n",
    "            fp.write(\"%d\\t%f\\t%f\\t%f\\t%f\\n\" % (i, loss[i], acc[i], val_loss[i], val_acc[i]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_fineTuning.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}